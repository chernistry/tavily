{
  "cells": [
    {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Tavily Web Scraper – Colab Notebook\n",
    "\n",
    "This notebook is the main UI for the Tavily Web Research Engineer assignment.\n",
    "\n",
    "It demonstrates a **hybrid scraping pipeline** over ~1k–10k mixed static and JS-heavy URLs:\n",
    "\n",
    "- Stage 1: async `httpx` fast path for static / mostly-static pages.\n",
    "- Stage 2: `playwright` (Chromium, headless) fallback for dynamic / blocked pages.\n",
    "\n",
    "Install dependencies, configure environment, and upload input files. Run the batch scraper via `run_all`, then analyze results through visualizations and metrics.\n",
    "\n",
    "> **Tip:** If you're viewing this on Colab from GitHub, the working directory is set up below so imports from `tavily_scraper` work out of the box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clone repository (Colab only)\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/chernistry/tavily.git\n",
    "    %cd tavily\n",
    "    print(\"Repository cloned and working directory set to /content/tavily\")\n",
    "else:\n",
    "    print(\"Running locally, assuming repository is already present\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Environment & path setup\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "repo_root = Path.cwd()\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(f\"Running in Colab, repo root: {repo_root}\")\n",
    "else:\n",
    "    print(f\"Running locally, CWD: {repo_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Install dependencies (run once per fresh Colab session)\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Use %pip so the environment is updated in the current kernel.\n",
    "    %pip install -q -r requirements.txt\n",
    "    # Install Chromium for Playwright (JS-enabled browser automation).\n",
    "    !python -m playwright install --with-deps chromium\n",
    "else:\n",
    "    print(\"Assuming dependencies are already installed in the local environment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configure environment and data paths\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from tavily_scraper.utils.io import ensure_canonical_urls_file\n",
    "\n",
    "base_data_dir = Path(\"/content/data\" if IN_COLAB else \"data\").resolve()\n",
    "base_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Core environment flags for the pipeline\n",
    "os.environ[\"TAVILY_ENV\"] = \"colab\" if IN_COLAB else \"local\"\n",
    "os.environ[\"TAVILY_DATA_DIR\"] = str(base_data_dir)\n",
    "os.environ.setdefault(\"HTTPX_TIMEOUT_SECONDS\", \"10\")\n",
    "os.environ.setdefault(\"HTTPX_MAX_CONCURRENCY\", \"32\")\n",
    "os.environ.setdefault(\"PLAYWRIGHT_HEADLESS\", \"true\")\n",
    "os.environ.setdefault(\"PLAYWRIGHT_MAX_CONCURRENCY\", \"2\")\n",
    "os.environ.setdefault(\"SHARD_SIZE\", \"500\")\n",
    "\n",
    "# Use files from repository .sdd/raw/ directory\n",
    "repo_raw_dir = Path(\".sdd/raw\")\n",
    "urls_csv_path = repo_raw_dir / \"urls.csv\"\n",
    "proxy_json_path = repo_raw_dir / \"proxy.json\"\n",
    "\n",
    "urls_txt_path = base_data_dir / \"urls.txt\"\n",
    "\n",
    "if urls_csv_path.exists():\n",
    "    urls_txt_path = ensure_canonical_urls_file(urls_csv_path, urls_txt_path)\n",
    "    print(f\"Loaded URLs from {urls_csv_path} -> {urls_txt_path}\")\n",
    "else:\n",
    "    print(f\"Warning: {urls_csv_path} not found in repository\")\n",
    "\n",
    "if proxy_json_path.exists():\n",
    "    proxy_dst = base_data_dir / \"proxy.json\"\n",
    "    shutil.copy(proxy_json_path, proxy_dst)\n",
    "    os.environ[\"PROXY_CONFIG_PATH\"] = str(proxy_dst)\n",
    "    print(f\"Loaded proxy config from {proxy_json_path} -> {proxy_dst}\")\n",
    "else:\n",
    "    print(f\"Warning: {proxy_json_path} not found in repository\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load configuration and inspect\n",
    "\n",
    "from tavily_scraper.config.env import load_run_config\n",
    "\n",
    "config = load_run_config()\n",
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the hybrid scraper (HTTPX first, optional Playwright fallback)\n",
    "\n",
    "from tavily_scraper.pipelines.batch_runner import run_all\n",
    "\n",
    "# Tune these for your run. For the assignment, target ~1,000 successful URLs.\n",
    "TARGET_SUCCESS = 1000\n",
    "USE_BROWSER = True  # set to False to run HTTP-only\n",
    "\n",
    "run_summary = await run_all(\n",
    "    config,\n",
    "    target_success=TARGET_SUCCESS,\n",
    "    use_browser=USE_BROWSER,\n",
    ")\n",
    "run_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load per-URL stats and run summary from disk\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tavily_scraper.utils.io import read_stats_jsonl\n",
    "\n",
    "data_dir = config.data_dir\n",
    "stats_path = data_dir / \"stats.jsonl\"\n",
    "summary_path = data_dir / \"run_summary.json\"\n",
    "\n",
    "stats_rows = read_stats_jsonl(stats_path)\n",
    "df = pd.DataFrame(stats_rows)\n",
    "print(f\"Loaded {len(df)} UrlStats rows from {stats_path}\")\n",
    "\n",
    "run_summary_from_disk = json.loads(summary_path.read_text(encoding=\"utf-8\"))\n",
    "run_summary_from_disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd85b972",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualizations: latency distributions and status breakdowns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if df.empty:\n",
    "    raise RuntimeError(\"No stats loaded – run the scraper cell first.\")\n",
    "\n",
    "httpx_df = df[df[\"method\"] == \"httpx\"]\n",
    "playwright_df = df[df[\"method\"] == \"playwright\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].hist(httpx_df[\"latency_ms\"].dropna(), bins=40, color=\"tab:blue\", alpha=0.8)\n",
    "axes[0].set_title(\"HTTPX latency (ms)\")\n",
    "axes[0].set_xlabel(\"Latency (ms)\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "axes[1].hist(\n",
    "    playwright_df[\"latency_ms\"].dropna(),\n",
    "    bins=40,\n",
    "    color=\"tab:orange\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "axes[1].set_title(\"Playwright latency (ms)\")\n",
    "axes[1].set_xlabel(\"Latency (ms)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "df[\"status\"].value_counts().plot(kind=\"bar\", title=\"Status distribution\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-domain status breakdown (top domains by total URLs)\n",
    "print(\"\\n=== Top 20 Domains by Status ===\")\n",
    "domain_status = df.groupby([\"domain\", \"status\"]).size().unstack(fill_value=0)\n",
    "# Sort by total URLs per domain\n",
    "domain_status[\"total\"] = domain_status.sum(axis=1)\n",
    "top_domains = domain_status.sort_values(\"total\", ascending=False).head(20)\n",
    "# Drop total column for display\n",
    "display_cols = [col for col in top_domains.columns if col != \"total\"]\n",
    "print(top_domains[display_cols].to_string())\n",
    "\n",
    "# Visualize top error domains\n",
    "if \"http_error\" in domain_status.columns:\n",
    "    error_domains = domain_status.sort_values(\"http_error\", ascending=False).head(10)\n",
    "    error_domains[\"http_error\"].plot(kind=\"barh\", title=\"Top 10 Domains by HTTP Errors\", color=\"#ef4444\")\n",
    "    plt.xlabel(\"Error Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "content-len-hist",
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Content length distribution by method (log x-axis)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "if df.empty:\n",
          "    raise RuntimeError(\"No stats loaded – run the scraper cell first.\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "\n",
        "for ax, subset, title, color in [\n",
        "    (axes[0], httpx_df, \"HTTPX content length\", \"tab:blue\"),\n",
        "    (axes[1], playwright_df, \"Playwright content length\", \"tab:orange\"),\n",
        "]:\n",
        "    # Consider only rows where we actually have meaningful content\n",
        "    subset_ok = subset[subset[\"status\"].isin([\"success\", \"too_large\"])]\n",
        "    sizes = subset_ok[\"content_len\"].dropna()\n",
        "    sizes = sizes[sizes > 0]\n",
        "    if sizes.empty:\n",
        "        continue\n",
        "    bins = np.logspace(np.log10(sizes.min()), np.log10(sizes.max()), 40)\n",
        "    ax.hist(sizes, bins=bins, color=color, alpha=0.8)\n",
        "    ax.set_xscale(\"log\")\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"Content length (bytes, log)\")\n",
        "    ax.set_ylabel(\"Count\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
