{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Tavily Web Scraper \u2013 Colab Notebook\n",
    "\n",
    "This notebook is the main UI for the Tavily Web Research Engineer assignment.\n",
    "\n",
    "It demonstrates a **hybrid scraping pipeline** over ~1k\u201310k mixed static and JS-heavy URLs:\n",
    "\n",
    "- Stage 1: async `httpx` fast path for static / mostly-static pages.\n",
    "- Stage 2: `playwright` (Chromium, headless) fallback for dynamic / blocked pages.\n",
    "\n",
    "Install dependencies, configure environment, and upload input files. Run the batch scraper via `run_all`, then analyze results through visualizations and metrics.\n",
    "\n",
    "> **Tip:** If you're viewing this on Colab from GitHub, the working directory is set up below so imports from `tavily_scraper` work out of the box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clone repository (Colab only)\n",
    "\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/chernistry/tavily.git\n",
    "    %cd tavily\n",
    "    print(\"Repository cloned and working directory set to /content/tavily\")\n",
    "else:\n",
    "    print(\"Running locally, assuming repository is already present\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Environment & path setup\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "repo_root = Path.cwd()\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(f\"Running in Colab, repo root: {repo_root}\")\n",
    "else:\n",
    "    print(f\"Running locally, CWD: {repo_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Install dependencies (run once per fresh Colab session)\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Use %pip so the environment is updated in the current kernel.\n",
    "    %pip install -q -r requirements.txt\n",
    "    # Install Chromium for Playwright (JS-enabled browser automation).\n",
    "    !python -m playwright install --with-deps chromium\n",
    "else:\n",
    "    print(\"Assuming dependencies are already installed in the local environment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configure environment and data paths\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from tavily_scraper.utils.io import ensure_canonical_urls_file\n",
    "\n",
    "base_data_dir = Path(\"/content/data\" if IN_COLAB else \"data\").resolve()\n",
    "base_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Core environment flags for the pipeline\n",
    "os.environ[\"TAVILY_ENV\"] = \"colab\" if IN_COLAB else \"local\"\n",
    "os.environ[\"TAVILY_DATA_DIR\"] = str(base_data_dir)\n",
    "os.environ.setdefault(\"HTTPX_TIMEOUT_SECONDS\", \"10\")\n",
    "os.environ.setdefault(\"HTTPX_MAX_CONCURRENCY\", \"32\")\n",
    "os.environ.setdefault(\"PLAYWRIGHT_HEADLESS\", \"true\")\n",
    "os.environ.setdefault(\"PLAYWRIGHT_MAX_CONCURRENCY\", \"2\")\n",
    "os.environ.setdefault(\"SHARD_SIZE\", \"500\")\n",
    "\n",
    "# Use files from repository .sdd/raw/ directory\n",
    "repo_raw_dir = Path(\".sdd/raw\")\n",
    "urls_csv_path = repo_raw_dir / \"urls.csv\"\n",
    "proxy_json_path = repo_raw_dir / \"proxy.json\"\n",
    "\n",
    "urls_txt_path = base_data_dir / \"urls.txt\"\n",
    "\n",
    "if urls_csv_path.exists():\n",
    "    urls_txt_path = ensure_canonical_urls_file(urls_csv_path, urls_txt_path)\n",
    "    print(f\"Loaded URLs from {urls_csv_path} -> {urls_txt_path}\")\n",
    "else:\n",
    "    print(f\"Warning: {urls_csv_path} not found in repository\")\n",
    "\n",
    "if proxy_json_path.exists():\n",
    "    proxy_dst = base_data_dir / \"proxy.json\"\n",
    "    shutil.copy(proxy_json_path, proxy_dst)\n",
    "    os.environ[\"PROXY_CONFIG_PATH\"] = str(proxy_dst)\n",
    "    print(f\"Loaded proxy config from {proxy_json_path} -> {proxy_dst}\")\n",
    "else:\n",
    "    print(f\"Warning: {proxy_json_path} not found in repository\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load configuration and inspect\n",
    "\n",
    "from tavily_scraper.config.env import load_run_config\n",
    "\n",
    "config = load_run_config()\n",
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the hybrid scraper (HTTPX first, optional Playwright fallback)\n",
    "\n",
    "from tavily_scraper.pipelines.batch_runner import run_all\n",
    "\n",
    "# Tune these for your run. For the assignment, target ~1,000 successful URLs.\n",
    "TARGET_SUCCESS = 1000\n",
    "USE_BROWSER = True  # set to False to run HTTP-only\n",
    "\n",
    "run_summary = await run_all(\n",
    "    config,\n",
    "    target_success=TARGET_SUCCESS,\n",
    "    use_browser=USE_BROWSER,\n",
    ")\n",
    "run_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load per-URL stats and run summary from disk\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tavily_scraper.utils.io import read_stats_jsonl\n",
    "\n",
    "data_dir = config.data_dir\n",
    "stats_path = data_dir / \"stats.jsonl\"\n",
    "summary_path = data_dir / \"run_summary.json\"\n",
    "\n",
    "stats_rows = read_stats_jsonl(stats_path)\n",
    "df = pd.DataFrame(stats_rows)\n",
    "print(f\"Loaded {len(df)} UrlStats rows from {stats_path}\")\n",
    "\n",
    "run_summary_from_disk = json.loads(summary_path.read_text(encoding=\"utf-8\"))\n",
    "run_summary_from_disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd85b972",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualizations: high-level metrics and interactive charts\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = 'plotly_white'\n",
    "\n",
    "if df.empty:\n",
    "    raise RuntimeError('No stats loaded - run the scraper cell first.')\n",
    "\n",
    "httpx_df = df[df['method'] == 'httpx']\n",
    "playwright_df = df[df['method'] == 'playwright']\n",
    "\n",
    "# 1) Status distribution (what happened?)\n",
    "status_counts = df['status'].value_counts().reset_index()\n",
    "status_counts.columns = ['status', 'count']\n",
    "\n",
    "fig = px.bar(\n",
    "    status_counts,\n",
    "    x='status',\n",
    "    y='count',\n",
    "    title='Status distribution',\n",
    "    text='count',\n",
    ")\n",
    "fig.update_traces(textposition='outside')\n",
    "fig.update_layout(xaxis_title='Status', yaxis_title='Count')\n",
    "fig.show()\n",
    "\n",
    "# 2) Latency distributions by method (how fast?)\n",
    "latency_df = df.dropna(subset=['latency_ms'])\n",
    "if not latency_df.empty:\n",
    "    fig = px.histogram(\n",
    "        latency_df,\n",
    "        x='latency_ms',\n",
    "        color='method',\n",
    "        nbins=40,\n",
    "        barmode='overlay',\n",
    "        opacity=0.8,\n",
    "        title='Latency distribution by method',\n",
    "    )\n",
    "    fig.update_layout(xaxis_title='Latency (ms)', yaxis_title='Count')\n",
    "    fig.show()\n",
    "\n",
    "# 3) Latency percentiles from run summary (P50/P95)\n",
    "latency_rows: list[dict[str, object]] = []\n",
    "for method_label, p50_key, p95_key in [\n",
    "    ('HTTPX', 'p50_latency_httpx_ms', 'p95_latency_httpx_ms'),\n",
    "    ('Playwright', 'p50_latency_playwright_ms', 'p95_latency_playwright_ms'),\n",
    "]:\n",
    "    p50 = run_summary_from_disk.get(p50_key)\n",
    "    p95 = run_summary_from_disk.get(p95_key)\n",
    "    if p50 is None or p95 is None:\n",
    "        continue\n",
    "    latency_rows.append({'method': method_label, 'metric': 'P50', 'latency_ms': p50})\n",
    "    latency_rows.append({'method': method_label, 'metric': 'P95', 'latency_ms': p95})\n",
    "\n",
    "if latency_rows:\n",
    "    latency_summary_df = pd.DataFrame(latency_rows)\n",
    "    fig = px.bar(\n",
    "        latency_summary_df,\n",
    "        x='method',\n",
    "        y='latency_ms',\n",
    "        color='metric',\n",
    "        barmode='group',\n",
    "        title='Latency by method (P50 vs P95)',\n",
    "        labels={'latency_ms': 'Latency (ms)'},\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# 4) Per-domain status breakdown (top 20 by URL count)\n",
    "print('\n=== Top 20 Domains by Status ===')\n",
    "domain_status = df.groupby(['domain', 'status']).size().unstack(fill_value=0)\n",
    "domain_status['total'] = domain_status.sum(axis=1)\n",
    "top_domains = domain_status.sort_values('total', ascending=False).head(20)\n",
    "display_cols = [col for col in top_domains.columns if col != 'total']\n",
    "print(top_domains[display_cols].to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "content-len-hist",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Content length distribution by method (log x-axis)\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "if df.empty:\n",
    "    raise RuntimeError('No stats loaded - run the scraper cell first.')\n",
    "\n",
    "content_df = df[(df['content_len'].notna()) & (df['content_len'] > 0)].copy()\n",
    "\n",
    "if not content_df.empty:\n",
    "    fig = px.histogram(\n",
    "        content_df,\n",
    "        x='content_len',\n",
    "        color='method',\n",
    "        nbins=40,\n",
    "        barmode='overlay',\n",
    "        opacity=0.8,\n",
    "        title='Content length distribution by method',\n",
    "        labels={'content_len': 'Content length (bytes)'},\n",
    "    )\n",
    "    fig.update_xaxes(type='log')\n",
    "    fig.show()\n",
    "else:\n",
    "    print('No non-empty content rows found for content length analysis.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}